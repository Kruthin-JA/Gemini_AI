{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WwoiVoGMuU87"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-generativeai>=0.7.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "HDFDvxgyvXfx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "response = model.generate_content(\"code of factorial in python.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "OtXsdIPrx-dD",
        "outputId": "3e77dc4c-432e-48d5-eaf2-d7c5e122503a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def factorial(n):\n",
            "  \"\"\"\n",
            "  Calculates the factorial of a non-negative integer.\n",
            "\n",
            "  Args:\n",
            "    n: A non-negative integer.\n",
            "\n",
            "  Returns:\n",
            "    The factorial of n (n!), which is the product of all positive integers \n",
            "    less than or equal to n. Returns 1 if n is 0.\n",
            "    Raises a ValueError if n is negative.\n",
            "  \"\"\"\n",
            "  if n < 0:\n",
            "    raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
            "  elif n == 0:\n",
            "    return 1\n",
            "  else:\n",
            "    result = 1\n",
            "    for i in range(1, n + 1):\n",
            "      result *= i\n",
            "    return result\n",
            "\n",
            "# Example usage:\n",
            "print(factorial(5))  # Output: 120\n",
            "print(factorial(0))  # Output: 1\n",
            "\n",
            "try:\n",
            "    print(factorial(-1))  # Raises ValueError\n",
            "except ValueError as e:\n",
            "    print(e)  # Output: Factorial is not defined for negative numbers.\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Error Handling:**  Includes `ValueError` handling for negative inputs.  This is crucial for robust code.  The function now explicitly raises an exception if the input is invalid, providing a clear error message.\n",
            "* **Base Case:**  Handles the base case of `n == 0` correctly, returning 1.  The factorial of 0 is defined as 1.\n",
            "* **Clear Documentation:**  The docstring (`\"\"\"...\"\"\"`) clearly explains what the function does, its arguments, what it returns, and any exceptions it might raise. This is essential for making the code understandable and maintainable.\n",
            "* **Iterative Approach:** Uses an iterative approach (a `for` loop). This is generally more efficient than a recursive approach in Python for factorials, as it avoids the overhead of function calls.\n",
            "* **Readability:** Uses descriptive variable names (`result`, `i`).  The code is well-formatted and easy to follow.\n",
            "* **Conciseness:**  Avoids unnecessary complexity. The code is straightforward and efficient.\n",
            "* **Example Usage:** Provides clear examples of how to use the function, including the case where a `ValueError` is raised.  This makes it easy for others to understand how to use the function correctly and what to expect in different scenarios.\n",
            "\n",
            "This improved version is more robust, readable, and efficient than simpler implementations, making it suitable for a wider range of uses.  It also follows best practices for Python coding.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"what is large language model.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "3fH7izfoy3U0",
        "outputId": "3391b07e-ad8f-4833-effd-f26867a39ba0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Large Language Model (LLM) is a type of artificial intelligence (AI) model designed to understand and generate human language. Think of it as a computer program trained on a massive amount of text and code data, learning the patterns, relationships, and nuances of language.\n",
            "\n",
            "Here's a breakdown of the key aspects:\n",
            "\n",
            "*   **Large:**  The \"large\" in LLM refers to the size of the model, particularly the number of parameters (adjustable values within the model).  A larger model generally has more capacity to learn complex patterns and relationships in the data. Billions or even trillions of parameters are common.\n",
            "\n",
            "*   **Language:**  LLMs are trained to understand and generate human language, encompassing various aspects like:\n",
            "    *   **Grammar and Syntax:**  Understanding the structure of sentences.\n",
            "    *   **Semantics:**  Grasping the meaning of words and phrases.\n",
            "    *   **Context:**  Recognizing the surrounding information to interpret language accurately.\n",
            "    *   **Nuance:**  Detecting subtle differences in meaning, tone, and intent.\n",
            "\n",
            "*   **Model:**  In machine learning, a \"model\" is a mathematical representation of a system learned from data.  LLMs are typically based on neural networks, specifically transformer architectures. These architectures are particularly well-suited for processing sequential data like text.\n",
            "\n",
            "**How they work:**\n",
            "\n",
            "1.  **Training Data:** LLMs are trained on vast datasets containing text and code from diverse sources, such as books, articles, websites, and code repositories.\n",
            "\n",
            "2.  **Learning Patterns:** During training, the model learns to predict the next word in a sequence, given the preceding words.  This process involves adjusting the model's parameters to minimize the error between its predictions and the actual text in the training data.\n",
            "\n",
            "3.  **Generating Text:**  Once trained, the LLM can generate new text by predicting the next word in a sequence, based on an initial prompt or context.  It repeats this process, building up text one word at a time.  The model's ability to generate coherent and relevant text depends on the quality and quantity of its training data and the sophistication of its architecture.\n",
            "\n",
            "**Key Characteristics & Capabilities:**\n",
            "\n",
            "*   **Text Generation:**  Creating new text in various styles and formats (e.g., articles, poems, code).\n",
            "*   **Text Summarization:** Condensing large amounts of text into shorter, informative summaries.\n",
            "*   **Translation:** Converting text from one language to another.\n",
            "*   **Question Answering:** Providing answers to questions based on the information it has learned.\n",
            "*   **Code Generation:**  Writing code in various programming languages.\n",
            "*   **Chatbots:** Powering conversational AI systems.\n",
            "*   **Content Creation:** Assisting with writing blog posts, marketing copy, and other content.\n",
            "*   **Sentiment Analysis:**  Identifying the emotional tone of a piece of text.\n",
            "*   **Natural Language Understanding (NLU):** Interpreting the meaning and intent behind human language.\n",
            "\n",
            "**Examples of LLMs:**\n",
            "\n",
            "*   **GPT (Generative Pre-trained Transformer) series (e.g., GPT-3, GPT-4) by OpenAI:**  Known for its impressive text generation capabilities.\n",
            "*   **LaMDA (Language Model for Dialogue Applications) by Google:**  Designed for conversational AI.\n",
            "*   **BERT (Bidirectional Encoder Representations from Transformers) by Google:** Widely used for NLU tasks like question answering and sentiment analysis.\n",
            "*   **LLaMA (Large Language Model Meta AI) by Meta:** An open-source LLM.\n",
            "*   **PaLM (Pathways Language Model) by Google:** Known for its reasoning and problem-solving abilities.\n",
            "\n",
            "**Limitations:**\n",
            "\n",
            "*   **Bias:** LLMs can inherit biases from the data they are trained on, leading to unfair or discriminatory outputs.\n",
            "*   **Lack of Real-World Understanding:**  They are trained on text and code, not on real-world experiences.  This can lead to inaccuracies or nonsensical outputs.\n",
            "*   **Factuality Issues (Hallucinations):** LLMs can sometimes generate incorrect or fabricated information.\n",
            "*   **Ethical Concerns:** Potential for misuse in generating misinformation, propaganda, or malicious content.\n",
            "*   **Resource Intensive:** Training and running LLMs requires significant computational resources and energy.\n",
            "*   **Over-reliance on Data:** Performance can be heavily affected by the quality and nature of the training data.\n",
            "\n",
            "**In summary, a Large Language Model is a powerful AI tool that leverages massive datasets and advanced neural network architectures to understand and generate human language.  While they offer impressive capabilities, it's crucial to be aware of their limitations and potential ethical implications.**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "ohe91LE41_Ey"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\" # @param [\"gemini-1.5-flash-latest\",\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.0-pro-exp-02-05\"] {\"allow-input\":true, isTemplate: true}\n",
        "from IPython.display import Markdown\n",
        "response=client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the largest planet in our solar system?\",\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "kifplq3t2_lf",
        "outputId": "2ab88484-625a-4de2-a310-69c16e58aa8b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASHBKEzv3ehp",
        "outputId": "594cdfca-19be-4e5f-d117-49dd935876b6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "IMG = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\"\n",
        "img_bytes = requests.get(IMG).content\n",
        "img_path = pathlib.Path(\"jetpack.png\")\n",
        "img_path.write_bytes(img_bytes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqvp-qnW4yK7",
        "outputId": "d35f5fdc-7e29-453f-be45-54a3049cf047"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1567837"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M8zTUIi17CG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}